<!DOCTYPE html>

<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
    <meta charset="utf-8">
<!--[if IE]><meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'><![endif]-->
<meta name="viewport" content="width=device-width,initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>2019/12/23-29 weekly review | SingularityKChen</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="2019/12/23-29 weekly review" />
<meta name="author" content="SingularityKChen" />
<meta property="og:locale" content="en" />
<meta name="description" content="the weekly review from 2019/12/23 to 29" />
<meta property="og:description" content="the weekly review from 2019/12/23 to 29" />
<link rel="canonical" href="http://localhost:4000/blog/2019/12/29/2019-12-23-29-weekly-review/" />
<meta property="og:url" content="http://localhost:4000/blog/2019/12/29/2019-12-23-29-weekly-review/" />
<meta property="og:site_name" content="SingularityKChen" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-29T00:00:00+08:00" />
<script type="application/ld+json">
{"dateModified":"2019-12-29T00:00:00+08:00","datePublished":"2019-12-29T00:00:00+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2019/12/29/2019-12-23-29-weekly-review/"},"url":"http://localhost:4000/blog/2019/12/29/2019-12-23-29-weekly-review/","author":{"@type":"Person","name":"SingularityKChen"},"description":"the weekly review from 2019/12/23 to 29","headline":"2019/12/23-29 weekly review","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<meta name="keywords" content="WeeklyReview,Eyeriss,Eyexam,DSH,DNN,EyerissV2" />





<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="SingularityKChen" />
    <link href='/assets/stylesheets/blog.css' rel="stylesheet" type="text/css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.min.js"></script>
<script>window.Modernizr || document.write('<script src="/assets/javascripts/modernizr-2.8.3.min.js"><\/script>')</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
    window.jQuery || document.write('<script src="/assets/javascripts/jquery-3.3.1.min.js"><\/script>')
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>
<script>
    window.Pace || document.write('<script src="/assets/javascripts/pace.min.js"><\/script>')
</script>

    
</head>

<body>
    

    <!--[if IE]>
    <p class="site-notice">You are using an outdated browser. Please <a href="http://browsehappy.com/" target="_blank">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true" target="_blank">activate Google Chrome Frame</a> to improve your experience.</p>
<![endif]-->
<noscript>
    <p class="site-notice">This site requires JavaScript. Here are the instructions <a href="http://www.enable-javascript.com/" target="_blank">how to enable JavaScript in your web browser</a>.</p>
</noscript>

    <div class="nav-wrapper overlay-wrapper">
    <div class="nav-form overlay-form">
        <span class="overlay-header menu">Menu</span>
        <a class="btn-close">Close</a>
        <div class="results">
            <ul>
                <li><a href="/blog/categories/">Categories</a></li>
                <li><a href="/blog/tags/">Tags</a></li>
                <li><a href="/">About</a></li>
            </ul>
        </div>
    </div>
</div>

<div class="search-wrapper overlay-wrapper">
    <div class="search-form overlay-form">
        <input type="text" class="overlay-header search-field" placeholder="Search...">
        <a class="btn-close">Close</a>
        <ul class="results"></ul>
    </div>
</div>


    <div id="page" class="hentry">
        <header class="the-header">
    <div class="unit-head">
        <div class="unit-inner unit-head-inner">
            <nav class="nav-global">
                <ul>
                    <li class="logo nav-link">
                        <button class="btn-menu" title="Menu"></button>
                        <a href="/blog/">SingularityKChen's Blog</a>
                        <span style="opacity: 0.6;">/<small><a href="https://github.com/SingularityKChen?tab=repositories">Projects</a></small></span>
                    </li>
                    <li class="nav-link"><a title="Categories" href="/blog/categories/">Categories</a></li>
                    <li class="nav-link"><a title="Tags" href="/blog/tags/">Tags</a></li>
                    <!--[if !IE]> -->
                    <li class="nav-link"><a title="Search" class="btn-search" href="#">Search</a></li>
                    <!-- <![endif]-->
                </ul>
            </nav>
        </div>
    </div>
</header>


        <div class="body animated fadeInDown" role="main">
            <div class="unit-body">
                <div class="unit-inner unit-body-inner">
                    <div class="entry-content">
                        <article class="unit-article layout-post">
    <div class="unit-inner unit-article-inner">
        <div itemscope itemtype="http://schema.org/Article" class="content">
            <header>
                <div class="unit-head">
                    <div class="unit-inner unit-head-inner">
                        <h1 class="entry-title" itemprop="name">2019/12/23-29 weekly review</h1>
                    </div>
                </div>
            </header>
            <div class="bd article-content">
                <div class="entry-content">
                    <div class="meta">
                        <p class="date-publish">
                            Published:
                            <time itemprop="datePublished" class="date-pub updated"
                                title="2019-12-29T00:00:00+08:00" datetime="2019-12-29T00:00:00+08:00">December 29, 2019 </time>
                            by
                            <a class="author" href="/" rel="author" title="Show Author">
                                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                                    <span itemprop="name">SingularityKChen</span>
                                </span>
                            </a>
                            
                                <a class="license-icon" rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" title="Show License">
                                    <img alt="Creative Commons Licence" style="border-width:0" src="/assets/images/theme/cc-by-sa.png"  height="16" width="80"/>
                                </a>
                            
                            
                        </p>
                        <ul class="list-category list-linear">
                            <li class="list-head">Categories: </li>
                             
     
        <li>
            <a href="/blog/categories/#WeeklyReview" title="WeeklyReview">
            WeeklyReview <span>8</span></a>
        </li>
    



                        </ul>
                        <ul class="list-tag list-linear">
                            <li class="list-head">Tags: </li>
                             
    
        
        <li>
            <a href="/blog/tags/#DNN" title="DNN">DNN <span>2</span></a>
        </li>
    
        
        <li>
            <a href="/blog/tags/#DSH" title="DSH">DSH <span>2</span></a>
        </li>
    
        
        <li>
            <a href="/blog/tags/#Eyeriss" title="Eyeriss">Eyeriss <span>2</span></a>
        </li>
    
        
        <li>
            <a href="/blog/tags/#EyerissV2" title="EyerissV2">EyerissV2 <span>2</span></a>
        </li>
    
        
        <li>
            <a href="/blog/tags/#Eyexam" title="Eyexam">Eyexam <span>1</span></a>
        </li>
    
        
        <li>
            <a href="/blog/tags/#WeeklyReview" title="WeeklyReview">WeeklyReview <span>8</span></a>
        </li>
    




                        </ul>
                    </div>
                    <div itemprop="articleBody">
                        <ul class="toc" id="markdown-toc">
  <li><a href="#heading-20191223-29" id="markdown-toc-heading-20191223-29">2019/12/23-29</a>    <ul>
      <li><a href="#heading-efficient-processing-of-deep-neural-networks-a-tutorial-and-survey" id="markdown-toc-heading-efficient-processing-of-deep-neural-networks-a-tutorial-and-survey">Efficient Processing of Deep Neural Networks: A Tutorial and Survey</a>        <ul>
          <li><a href="#heading-creating-a-system-for-efficient-dnn-processing" id="markdown-toc-heading-creating-a-system-for-efficient-dnn-processing">Creating A System for Efficient DNN Processing</a></li>
          <li><a href="#heading-temporal-architectures-and-spatial-architectures" id="markdown-toc-heading-temporal-architectures-and-spatial-architectures">Temporal Architectures and Spatial Architectures</a></li>
          <li><a href="#heading-near-data-processing" id="markdown-toc-heading-near-data-processing">Near-Data Processing</a></li>
          <li><a href="#heading-co-design-of-dnn-models-and-hardware" id="markdown-toc-heading-co-design-of-dnn-models-and-hardware">Co-Design of DNN Models And Hardware</a></li>
          <li><a href="#heading-benchmarking-metrics-for-dnn-evaluation-and-comparison" id="markdown-toc-heading-benchmarking-metrics-for-dnn-evaluation-and-comparison">Benchmarking Metrics for DNN Evaluation and Comparison</a></li>
        </ul>
      </li>
      <li><a href="#heading-eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks" id="markdown-toc-heading-eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks">Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks</a>        <ul>
          <li><a href="#heading-changes-in-performance-and-flexibility" id="markdown-toc-heading-changes-in-performance-and-flexibility">Changes in Performance and Flexibility</a></li>
          <li><a href="#heading-eyexam" id="markdown-toc-heading-eyexam">Eyexam</a></li>
          <li><a href="#heading-flexible-dataflow" id="markdown-toc-heading-flexible-dataflow">Flexible Dataflow</a></li>
          <li><a href="#heading-flexible-and-scalable-noc" id="markdown-toc-heading-flexible-and-scalable-noc">Flexible and Scalable NoC</a></li>
        </ul>
      </li>
      <li><a href="#heading-eyeriss-a-spatial-architecture-for-energy-efficient-dataflow-for-convolutional-neural-networks" id="markdown-toc-heading-eyeriss-a-spatial-architecture-for-energy-efficient-dataflow-for-convolutional-neural-networks">Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks</a>        <ul>
          <li><a href="#heading-framework-for-energy-efficiency-analysis" id="markdown-toc-heading-framework-for-energy-efficiency-analysis">Framework for Energy Efficiency Analysis</a></li>
        </ul>
      </li>
      <li><a href="#heading-eyeriss-an-energy-efficient-reconfigurable-accelerator-for-deep-convolutional-neural-networks" id="markdown-toc-heading-eyeriss-an-energy-efficient-reconfigurable-accelerator-for-deep-convolutional-neural-networks">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</a>        <ul>
          <li><a href="#heading-main-features-of-eyeriss" id="markdown-toc-heading-main-features-of-eyeriss">Main Features of Eyeriss</a></li>
          <li><a href="#heading-system-control-and-configuration" id="markdown-toc-heading-system-control-and-configuration">System Control and Configuration</a></li>
          <li><a href="#heading-exploit-data-statistics" id="markdown-toc-heading-exploit-data-statistics">Exploit Data Statistics</a></li>
        </ul>
      </li>
      <li><a href="#heading-some-troubles" id="markdown-toc-heading-some-troubles">Some Troubles</a>        <ul>
          <li><a href="#heading-scalac_2124423" id="markdown-toc-heading-scalac_2124423">Scalac_2.12.4:4.2.3</a></li>
          <li><a href="#heading-no-found-firrlt-object" id="markdown-toc-heading-no-found-firrlt-object">No found: firrlt object</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h1 id="heading-20191223-29">2019/12/23-29</h1>

<p>Last week, I read four Journal Articles related to Deep Learning Accelerator Implementation. Thanks to Shien's recommendation, I learned a lot from these articles as I have a more vivid knowledge of DLA with several in-deep views in some components.</p>

<p>When I reviewed these four articles, I found that even though I know how to design the hardware architecture, I may not know how to run some DNN models with the accelerator. Letty told me that they need the help of some compilers to translate the models to instructions, but I don't think I could optimize the compiler to compile the program into my custom instructions. So maybe I would not run a complex model if I can not load instructions manually.</p>

<p>And next week, I am supposed to read some papers which describe the method to compress the different types of data in CNN, i.e., QNN, to decrease the movements of data and the storage requirements of its four levels of the memory hierarchy. Besides, Chen Peng has introduced me thinking that routers can also be regards as one of the calculation components in NoC, so maybe the next few weeks, I will improve the dataflow of Eyeriss with this kind of pattern.</p>

<hr />

<h2 id="heading-efficient-processing-of-deep-neural-networks-a-tutorial-and-survey">Efficient Processing of Deep Neural Networks: A Tutorial and Survey</h2>

<p>This article focuses on:</p>

<ul>
  <li>processing of DNN inference</li>
  <li>addressing the efficiency of the CONV layers</li>
</ul>

<h3 id="heading-creating-a-system-for-efficient-dnn-processing">Creating A System for Efficient DNN Processing</h3>

<ul>
  <li>Applications and specific computations requirements</li>
  <li>Understand and balance the important system metrics
    <ul>
      <li>Accuracy
        <ul>
          <li>Energy</li>
          <li>Throughput</li>
          <li>Hardware cost</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimize DNN processing</li>
  <li>Joint hardware/software co-design</li>
</ul>

<h3 id="heading-temporal-architectures-and-spatial-architectures">Temporal Architectures and Spatial Architectures</h3>

<p><img src="https://images-cdn.shimo.im/Y0FJBfLpHro9PDzw/image.png" alt="Temporal Architectures and Spatial Architectures" /></p>

<h4 id="heading-temporal-architectures">Temporal Architectures</h4>

<ul>
  <li>appear mostly in CPUs or GPUs</li>
  <li>employ a variety of techniques to improve parallelisms such as vectors (SIMD) or parallel threads (SIMT)</li>
  <li>use a centralized control for a large number of ALUs</li>
  <li>can only fetch data from the memory hierarchy</li>
  <li>cannot communicate directly with each other</li>
  <li>reduce the number of multiplications to increase throughput</li>
</ul>

<h4 id="heading-spatial-architectures">Spatial Architectures</h4>

<ul>
  <li>use dataflow processing can pass data from one to another directly</li>
  <li>ALU can have its control logic and local memory</li>
  <li>how dataflows can increase data reuse from low-cost memories in the memory hierarchy to reduce energy consumption</li>
</ul>

<h4 id="heading-apply-computational-transforms-to-the-data">Apply Computational Transforms to The Data</h4>

<ul>
  <li>Fast Fourier Transform (FFT) from O( $N^2_o$ $N^2_f$ ) to O($N^2_o$ $log_2N_o$)</li>
  <li>Winograd’s algorithm</li>
  <li>Strassen’s algorithm from O($N^3$) to O($N^{2.807}$)</li>
</ul>

<h4 id="heading-energy-efficient-dataflow-for-accelerators">Energy-Efficient Dataflow for Accelerators</h4>

<p>Each MAC requires three memory reads:</p>

<ul>
  <li>filter weight</li>
  <li>fmap activation</li>
  <li>partial sum</li>
</ul>

<p>And one memory write: updated partial sum.</p>

<p>Different Dataflow:</p>

<ul>
  <li>Weight Stationary <em>(WS)</em></li>
  <li>Output Stationary <em>(OS)</em></li>
  <li>No Local Reuse <em>(NLR)</em></li>
  <li>Row Stationary <em>(RS)</em></li>
</ul>

<h3 id="heading-near-data-processing">Near-Data Processing</h3>

<p>We are supported to reduce data movement by moving compute and data closer. For example,</p>

<ul>
  <li>Integrate the computation into the memory itself</li>
  <li>Bring the computation into the sensor where the data are first collected</li>
</ul>

<h4 id="heading-dram">DRAM</h4>

<p>Avoid off-chip access by using high-density memories such as DRAMs. which can store tens of megabytes of weights and activations on-chip.</p>

<p>Also, with the help of through-silicon vias <em>(TSVs)</em>, or 3-D memory, and HMC, HBM, the DRAM can be stacked on the top of the chip.</p>

<h4 id="heading-sram">SRAM</h4>

<p>Bring the compute into the memory.</p>

<h4 id="heading-non-volatile-resistive-memories">Non-volatile Resistive Memories</h4>

<ul>
  <li>resistor’s conductance as the weight</li>
  <li>the voltage as the input</li>
  <li>the current as the output</li>
  <li>the addition is done by Kirchhoff’s current law</li>
</ul>

<p>But it has some cons:</p>

<ul>
  <li>suffers from the reduced precision as it needs ADC/DAC</li>
  <li>the array size is limited by the wires</li>
  <li>the IR drop along wire can degrade the read accuracy</li>
</ul>

<h4 id="heading-sensors">Sensors</h4>

<p>Need to move the computation into the analogy domain to avoid using the ADC within the sensor.</p>

<h3 id="heading-co-design-of-dnn-models-and-hardware">Co-Design of DNN Models And Hardware</h3>

<h4 id="heading-reduce-precision">Reduce Precision</h4>

<p>And we can apply different precisions into weights and activations, different layers.</p>

<p>Reduce the precision of operations and operands:</p>

<ul>
  <li>fixed point instead of floating-point</li>
  <li>reducing the bit-width: uniform quantization</li>
  <li>nonuniform quantization: map data to a smaller set of quantization levels
    <ul>
      <li>log quantization
        <ul>
          <li>learned quantization</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>weight sharing</li>
</ul>

<h4 id="heading-reduced-number-of-operations-and-model-size">Reduced Number of Operations and Model Size</h4>

<p>Reduce the number of operations and model size:</p>

<ul>
  <li>compression: exploiting activation statistics</li>
  <li>network pruning</li>
  <li>compact network architectures: replace a large filter with a series of small filters</li>
</ul>

<h3 id="heading-benchmarking-metrics-for-dnn-evaluation-and-comparison">Benchmarking Metrics for DNN Evaluation and Comparison</h3>

<h4 id="heading-metrics-for-dnn-models">Metrics for DNN Models</h4>

<ul>
  <li>The accuracy</li>
  <li>The network architecture of the model including the number of layers, filter sizes, number of filters and number of channels</li>
  <li>The number of weights: impact the storage requirement</li>
  <li>The number of MACs: potential throughput</li>
</ul>

<h4 id="heading-metrics-for-dnn-hardware">Metrics for DNN Hardware</h4>

<ul>
  <li>The power  and energy consumption</li>
  <li>The latency and throughput</li>
  <li>The cost of the chip: the size and type of memory, the amount of control logic</li>
  <li>For an FPGA,
    <ul>
      <li>the specific device
        <ul>
          <li>the utilization of resources such as:
            <ul>
              <li>DSP
                <ul>
                  <li>BRAM</li>
                  <li>LUT</li>
                  <li>FF</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>performance density
            <ul>
              <li>GOPs/slice</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="heading-eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks">Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks</h2>

<p>This article describes:</p>

<ul>
  <li>a performance analysis framework named <em><code class="language-plaintext highlighter-rouge">Eyexam</code></em> which provides a seven-step process to systematically identify the source of performance loss and can be used to develop a set of <code class="language-plaintext highlighter-rouge">roofline models</code> to quantitatively identify the source of performance loss in DNN processors;</li>
  <li>a DNN accelerator named <em><code class="language-plaintext highlighter-rouge">Eyeriss v2</code></em> which uses a hierarchical <code class="language-plaintext highlighter-rouge">NoC</code> that allows the system to exploit spatial data reuse for large DNNs and deliver high bandwidth for compact DNNs with the scalability;</li>
</ul>

<h3 id="heading-changes-in-performance-and-flexibility">Changes in Performance and Flexibility</h3>

<h4 id="heading-two-bad-ways-in-widely-varying-data-reuse">Two Bad Ways in Widely Varying Data Reuse</h4>

<ul>
  <li>depend on data reuse in certain data dimensions
    <ul>
      <li>the spatial accumulation array architecture relies on both output and input channels;
        <ul>
          <li>the temporal accumulation array architecture relies on another set of data dimensions;</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>lower data reuse need a higher data bandwidth</li>
</ul>

<h4 id="heading-to-build-a-truly-flexible-dnn-processor">To Build a Truly Flexible DNN Processor</h4>

<ul>
  <li>the dataflow relies on certain data dimensions for data reuse
    <ul>
      <li>low utilization of the parallelism when those data dimensions diminish</li>
    </ul>
  </li>
  <li>the NoC and its corresponding memory hierarchy
    <ul>
      <li>high bandwidth and low spatial data reuse;
        <ul>
          <li>low bandwidth and high spatial data reuse scenarios;</li>
          <li>instead of being adaptive to the specific condition of the workload;</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="heading-eyexam">Eyexam</h3>

<h4 id="heading-two-main-factors-that-affect-performance">Two Main Factors that Affect Performance</h4>

<ul>
  <li>the number of active PEs due to the mapping determined by the dataflow;</li>
  <li>the utilization of active PEs based on whether the NoC has sufficient bandwidth to deliver data to PEs to keep them active;</li>
</ul>

<p>This is a example dataflow for a 1D CONV:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;stdio.h&gt;
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
		<span class="kt">int</span> <span class="n">R2</span><span class="p">,</span> <span class="n">R1</span><span class="p">,</span> <span class="n">R0</span><span class="p">,</span> <span class="n">E2</span><span class="p">,</span> <span class="n">E1</span><span class="p">,</span> <span class="n">E0</span><span class="p">;</span>
		<span class="c1">// R0, R1, R2 related to filter</span>
		<span class="n">R2</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="c1">//chnl of filter, C, sum of one chnl ofmap</span>
		<span class="n">R1</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span><span class="c1">//height of filter, R, psum of one row ofmap</span>
		<span class="n">R0</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span><span class="c1">//width of filter, S, ppsum of one ofmap</span>
		<span class="c1">// E0, E1, E2 related to ofmap</span>
		<span class="n">E2</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="c1">//chnl of ofmap or number of filters, M, sums in different chnl ofmaps</span>
		<span class="n">E1</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span><span class="c1">//height of ofmap, E, psum of one chnl ofmap</span>
		<span class="n">E0</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span><span class="c1">//width of ofmap, F, ppsum of one row ofmap</span>
		<span class="kt">int</span> <span class="n">inx_o</span><span class="p">,</span> <span class="n">inx_i</span><span class="p">,</span> <span class="n">inx_w</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">e2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">e2</span> <span class="o">&lt;</span> <span class="n">E2</span><span class="p">;</span> <span class="n">e2</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
				<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">r2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">r2</span> <span class="o">&lt;</span> <span class="n">R2</span><span class="p">;</span> <span class="n">r2</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
						<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">e1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">e1</span> <span class="o">&lt;</span> <span class="n">E1</span><span class="p">;</span> <span class="n">e1</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
								<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">r1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">r1</span> <span class="o">&lt;</span> <span class="n">R1</span><span class="p">;</span> <span class="n">r1</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
										<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">e0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">e0</span> <span class="o">&lt;</span> <span class="n">E0</span><span class="p">;</span> <span class="n">e0</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
												<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">r0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">r0</span> <span class="o">&lt;</span> <span class="n">R0</span><span class="p">;</span> <span class="n">r0</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
														<span class="n">inx_o</span> <span class="o">=</span> <span class="n">e2</span><span class="o">*</span><span class="n">E1</span><span class="o">*</span><span class="n">E0</span><span class="o">+</span><span class="n">e1</span><span class="o">*</span><span class="n">E0</span><span class="o">+</span><span class="n">e0</span><span class="p">;</span>
														<span class="n">inx_i</span> <span class="o">=</span> <span class="n">e2</span><span class="o">*</span><span class="n">E1</span><span class="o">*</span><span class="n">E0</span><span class="o">+</span><span class="n">e1</span><span class="o">*</span><span class="n">E0</span><span class="o">+</span><span class="n">e0</span> <span class="o">+</span> <span class="n">r2</span><span class="o">*</span><span class="n">R1</span><span class="o">*</span><span class="n">R0</span><span class="o">+</span><span class="n">r1</span><span class="o">*</span><span class="n">R0</span><span class="o">+</span><span class="n">r0</span><span class="p">;</span>
														<span class="n">inx_w</span> <span class="o">=</span> <span class="n">r2</span><span class="o">*</span><span class="n">R1</span><span class="o">*</span><span class="n">R0</span><span class="o">+</span><span class="n">r1</span><span class="o">*</span><span class="n">R0</span><span class="o">+</span><span class="n">r0</span><span class="p">;</span>
														<span class="n">printf</span><span class="p">(</span><span class="s">"O[%d] += I[%d] * W[%d]</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">inx_o</span><span class="p">,</span> <span class="n">inx_i</span><span class="p">,</span> <span class="n">inx_w</span><span class="p">);</span>
														<span class="c1">//printf("O[e2*E1*E0+e1*E0+e0] = O[%d*E1*E0+%d*E0+%d]\n", e2, e1, e0);</span>
														<span class="cm">/* code */</span>
												<span class="p">}</span>
												<span class="cm">/* code */</span>
												<span class="c1">//printf("----------- r0 for loop finished -----------\n");</span>
												<span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
										<span class="p">}</span>
										<span class="cm">/* code */</span>
										<span class="n">printf</span><span class="p">(</span><span class="s">"*********** e0 for loop finished ***********</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
								<span class="p">}</span>
								<span class="cm">/* code */</span>
								<span class="n">printf</span><span class="p">(</span><span class="s">"+++++++++++ r1 for loop finished +++++++++++</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
						<span class="p">}</span>
						<span class="cm">/* code */</span>
						<span class="n">printf</span><span class="p">(</span><span class="s">"=========== e1 for loop finished ===========</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
				<span class="p">}</span>
				<span class="cm">/* code */</span>
				<span class="n">printf</span><span class="p">(</span><span class="s">"@@@@@@@@@@@ r2 for loop finished @@@@@@@@@@@</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div>

<ul>
  <li>inner two <code class="language-plaintext highlighter-rouge">for</code> loops represent the temporal processing and <code class="language-plaintext highlighter-rouge">SPad</code> accesses within a <code class="language-plaintext highlighter-rouge">PE</code>;</li>
  <li>the outer two <code class="language-plaintext highlighter-rouge">for</code> loops represent the temporal processing of multiple passes across the <code class="language-plaintext highlighter-rouge">PE</code> array and GLB accesses;</li>
  <li>the two <code class="language-plaintext highlighter-rouge">parallel-for</code>s represent the distribution of computation across multiple <code class="language-plaintext highlighter-rouge">PE</code>s;</li>
</ul>

<h4 id="heading-seven-steps-to-analyse-the-performance-of-an-architecture">Seven steps to analyse the performance of an architecture</h4>

<ul>
  <li>layer shape and size: the number of <code class="language-plaintext highlighter-rouge">MAC</code>s in the layer</li>
  <li>dataflow: the maximum parallelism of the dataflow</li>
  <li>number of <code class="language-plaintext highlighter-rouge">PE</code>s: the theoretical peak performance. Two shape fragmentation:
    <ul>
      <li>spatial mapping fragmentation: <code class="language-plaintext highlighter-rouge">R</code> or <code class="language-plaintext highlighter-rouge">R1</code> is smaller than the number of <code class="language-plaintext highlighter-rouge">PE</code>s =&gt; some are completely idle
        <ul>
          <li>temporal mapping fragmentation: <code class="language-plaintext highlighter-rouge">R</code> is not an integer multiple of <code class="language-plaintext highlighter-rouge">PE</code>s =&gt; some are not always active</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>physical dimensions of the <code class="language-plaintext highlighter-rouge">PE</code> array: the run-time utilization of <code class="language-plaintext highlighter-rouge">PE</code>s due to shape fragmentation for each dimension</li>
  <li>storage capacity: reduce the number of active <code class="language-plaintext highlighter-rouge">PE</code>s due to storage of intermediate data. e.g., limited storage for <code class="language-plaintext highlighter-rouge">psum</code>s =&gt; the limited number of weights be processed in parallel =&gt; limited number of active <code class="language-plaintext highlighter-rouge">PE</code>s that can operate in parallel</li>
  <li>data bandwidth: insufficient average bandwidth to active <code class="language-plaintext highlighter-rouge">PE</code>s. The performance will be bandwidth-limited when the operational intensity is lower than the inflection point</li>
  <li>varying data access patterns: insufficient instant bandwidth to active <code class="language-plaintext highlighter-rouge">PE</code>s</li>
</ul>

<p><img src="https://images-cdn.shimo.im/C9YNzwIHSE87b2sJ/image.png" alt="the roofline model" /></p>

<p><img src="https://images-cdn.shimo.im/OJ7PwTSeEwIakuk9/image.png" alt="the impact of Eyexam steps on the roofline model" /></p>

<p><img src="https://images-cdn.shimo.im/baEhAdv31Z0zD03y/image.png" alt="summary of steps in Eyexam" /></p>

<h4 id="heading-performance-analysis-results-for-dnn-processors-and-workloads">Performance Analysis Results for DNN Processors and Workloads</h4>

<ul>
  <li>simply increasing hardware resources is not sufficient to achieve a higher performance</li>
  <li>the dataflow has to be flexible enough to deal the diminished reuse available in any data dimensions</li>
  <li>the NoC design should meet the worst-case bandwidth requirement for every data type</li>
  <li>the NoC design should aim to exploit data reuse to minimize the number of GLB accesses</li>
</ul>

<h3 id="heading-flexible-dataflow">Flexible Dataflow</h3>

<p>The new dataflow named <em><code class="language-plaintext highlighter-rouge">Row-Stationary Plus</code></em> supports data tiling from all dimensions to fully utilize the PE array.</p>

<ul>
  <li>additional loops <em><code class="language-plaintext highlighter-rouge">g1</code></em> and <em><code class="language-plaintext highlighter-rouge">n1</code></em> are added at the NoC level to provide more options to parallelize different dimensions of data</li>
  <li>allow data to be tiled from multiple dimensions and mapped in parallel onto the same <code class="language-plaintext highlighter-rouge">PE</code> array dimension</li>
  <li>the data tile from the same dimension can also be mapped spatially onto different physical dimensions to fully utilize the <code class="language-plaintext highlighter-rouge">PE</code> array</li>
  <li>allow tile in data dimension <em><code class="language-plaintext highlighter-rouge">R</code></em> with loop <em><code class="language-plaintext highlighter-rouge">r1</code></em></li>
  <li>an additional loop <em><code class="language-plaintext highlighter-rouge">e0</code></em> is added at the <code class="language-plaintext highlighter-rouge">SPad</code> level to create more reuse of weights in the <code class="language-plaintext highlighter-rouge">SPad</code>.</li>
</ul>

<p><img src="https://images-cdn.shimo.im/Lr26hZAgvUcKEqu7/image.png" alt="The definition of the Row-Stationary Plus (RS+) dataflow" /></p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inx_o</span> <span class="o">=</span> <span class="n">g3</span><span class="o">*</span><span class="n">G2</span><span class="o">*</span><span class="n">G1</span> <span class="o">+</span> <span class="n">g2</span><span class="o">*</span><span class="n">G1</span> <span class="o">+</span> <span class="n">g1</span>
				<span class="o">+</span> <span class="n">n3</span><span class="o">*</span><span class="n">N2</span><span class="o">*</span><span class="n">N1</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="n">n2</span><span class="o">*</span><span class="n">N1</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="n">n1</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="n">n0</span>
				<span class="o">+</span> <span class="n">m3</span><span class="o">*</span><span class="n">M2</span><span class="o">*</span><span class="n">M1</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">m2</span><span class="o">*</span><span class="n">M1</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">m1</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">m0</span>
				<span class="o">+</span> <span class="n">e0</span>
				<span class="o">+</span> <span class="n">f3</span><span class="o">*</span><span class="n">F2</span><span class="o">*</span><span class="n">F1</span><span class="o">*</span><span class="n">F0</span> <span class="o">+</span> <span class="n">f2</span><span class="o">*</span><span class="n">F1</span><span class="o">*</span><span class="n">F0</span> <span class="o">+</span> <span class="n">f1</span><span class="o">*</span><span class="n">F0</span> <span class="o">+</span> <span class="n">f0</span><span class="p">;</span>
<span class="n">inx_i</span> <span class="o">=</span> <span class="n">g3</span><span class="o">*</span><span class="n">G2</span><span class="o">*</span><span class="n">G1</span> <span class="o">+</span> <span class="n">g2</span><span class="o">*</span><span class="n">G1</span> <span class="o">+</span> <span class="n">g1</span>
				<span class="o">+</span> <span class="n">n3</span><span class="o">*</span><span class="n">N2</span><span class="o">*</span><span class="n">N1</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="n">n2</span><span class="o">*</span><span class="n">N1</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="n">n1</span><span class="o">*</span><span class="n">N0</span> <span class="o">+</span> <span class="n">n0</span>
				<span class="o">+</span> <span class="n">e0</span>
				<span class="o">+</span> <span class="n">f3</span><span class="o">*</span><span class="n">F2</span><span class="o">*</span><span class="n">F1</span><span class="o">*</span><span class="n">F0</span> <span class="o">+</span> <span class="n">f2</span><span class="o">*</span><span class="n">F1</span><span class="o">*</span><span class="n">F0</span> <span class="o">+</span> <span class="n">f1</span><span class="o">*</span><span class="n">F0</span> <span class="o">+</span> <span class="n">f0</span>
				<span class="o">+</span> <span class="n">c2</span><span class="o">*</span><span class="n">C1</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">c1</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">c0</span>
				<span class="o">+</span> <span class="n">r0</span>
				<span class="o">+</span> <span class="n">s2</span><span class="o">*</span><span class="n">S1</span> <span class="o">+</span> <span class="n">s1</span><span class="p">;</span>
<span class="n">inx_w</span> <span class="o">=</span> <span class="n">g3</span><span class="o">*</span><span class="n">G2</span><span class="o">*</span><span class="n">G1</span> <span class="o">+</span> <span class="n">g2</span><span class="o">*</span><span class="n">G1</span> <span class="o">+</span> <span class="n">g1</span>
				<span class="o">+</span> <span class="n">m3</span><span class="o">*</span><span class="n">M2</span><span class="o">*</span><span class="n">M1</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">m2</span><span class="o">*</span><span class="n">M1</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">m1</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">m0</span>
				<span class="o">+</span> <span class="n">c2</span><span class="o">*</span><span class="n">C1</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">c1</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">c0</span>
				<span class="o">+</span> <span class="n">r0</span>
				<span class="o">+</span> <span class="n">s2</span><span class="o">*</span><span class="n">S1</span> <span class="o">+</span> <span class="n">s1</span><span class="p">;</span>

<span class="n">inx_o</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">m0</span> <span class="o">+</span> <span class="n">e0</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">n0</span><span class="o">*</span><span class="n">E0</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">f0</span><span class="o">*</span><span class="n">N0</span><span class="o">*</span><span class="n">E0</span><span class="o">*</span><span class="n">M0</span><span class="p">;</span>
<span class="n">inx_w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">m0</span> <span class="o">+</span> <span class="n">c0</span><span class="o">*</span><span class="n">M0</span> <span class="o">+</span> <span class="n">r0</span><span class="o">*</span><span class="n">C0</span><span class="o">*</span><span class="n">M0</span><span class="p">;</span>
<span class="n">inx_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">c0</span> <span class="o">+</span> <span class="n">r0</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">e0</span><span class="o">*</span><span class="n">R0</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">n0</span><span class="o">*</span><span class="n">E0</span><span class="o">*</span><span class="n">R0</span><span class="o">*</span><span class="n">C0</span> <span class="o">+</span> <span class="n">f0</span><span class="o">*</span><span class="n">N0</span><span class="o">*</span><span class="n">E0</span><span class="o">*</span><span class="n">R0</span><span class="o">*</span><span class="n">C0</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="heading-flexible-and-scalable-noc">Flexible and Scalable NoC</h3>

<p>A new NoC named <em><code class="language-plaintext highlighter-rouge">hierarchical mesh</code></em> is designed to adapt to a wide range of bandwidth requirements.</p>

<h4 id="heading-the-pros-and-cons-of-different-noc-implementations">The Pros and Cons of Different NoC Implementations</h4>

<ul>
  <li>Broadcast Network: high reuse but low bandwidth</li>
  <li>Unicast Network: high bandwidth but low reuse</li>
  <li>All-to-All Network: implementation cost and energy consumption increase quadratically</li>
</ul>

<p><img src="https://images-cdn.shimo.im/otl1ciltLJkHZZql__thumbnail" alt="The pros and cons of different NoC implementations" /></p>

<h4 id="heading-architecture-and-work-models-of-hierarchical-mesh-network">Architecture and Work Models of Hierarchical Mesh Network</h4>

<p><img src="https://images-cdn.shimo.im/KGUO8tm73RYl12vk/image.png__thumbnail" alt="Architecture of Hierarchical Mesh Network" /></p>

<p><img src="https://images-cdn.shimo.im/xxbVBVUFHdkow9E3/image.png__thumbnail" alt="Work Models of Hierarchical Mesh Network" /></p>

<h4 id="heading-considerations">Considerations</h4>

<p>The size of clusters:</p>

<ul>
  <li>what is the bandwidth required from the source cluster in the worst case, i.e., unicast mode;</li>
  <li>what is the bandwidth required in between the router clusters in the worst case, i.e., interleaved multicast mode;</li>
  <li>what is the tolerable cost of the all-to-all network;</li>
</ul>

<p>The tile size of the mapped data dimensions:</p>

<ul>
  <li>the cluster size</li>
  <li>the number of clusters</li>
</ul>

<h2 id="heading-eyeriss-a-spatial-architecture-for-energy-efficient-dataflow-for-convolutional-neural-networks">Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks</h2>

<p>Compared to the <a href="#eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks">Eyeriss v2</a>, this article provides a more detailed explanation of <em>Row Stationary</em>, a baseline storage area for a given number of PEs and the energy cost estimation for RS reuse pattern.</p>

<hr />

<p>This article proposed RS dataflow which can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism.</p>

<p>Also, an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints.</p>

<h3 id="heading-framework-for-energy-efficiency-analysis">Framework for Energy Efficiency Analysis</h3>

<h4 id="heading-storage-area">Storage Area</h4>

<p>The baseline storage area for a given number of PEs is calculated as</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#PE×Area(512B RF)+Area((#PE×512B) global buffer).
</code></pre></div></div>

<p><img src="https://images-cdn.shimo.im/H6Ir275vy2cj9SXP/image.png" alt="" /></p>

<h4 id="heading-input-data-access-energy-cost">Input Data Access Energy Cost</h4>

<p>Input data access energy cost estimation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a×EC(DRAM)+ab×EC(global buffer)+abc×EC(array)+abcd×EC(RF)
</code></pre></div></div>

<p><img src="https://images-cdn.shimo.im/4HIcPqN7ZJYtzTZu/image.png" alt="Example of input data" /></p>

<h4 id="heading-psum-accumulation-energy-cost">Psum Accumulation Energy Cost</h4>

<p>Psum accumulation energy cost estimation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2a−1)×EC(DRAM)+2a(b−1)×EC(global buffer)+ab(c−1)×EC(array)+2abc(d−1)×EC(RF)
</code></pre></div></div>

<p><img src="https://images-cdn.shimo.im/YpjPqAj7BIkUqJOS/image.png" alt="Example of Psum accumulation" /></p>

<p>where the <code class="language-plaintext highlighter-rouge">EC(*)</code> is shown as follows:</p>

<p><img src="https://images-cdn.shimo.im/QQxUYwRr9kEkrpyE/image.png" alt="NORMALIZED ENERGY COST RELATIVE TO A MAC OPERATION EXTRACTED FROM A COMMERCIAL 65NM PROCESS" /></p>

<h2 id="heading-eyeriss-an-energy-efficient-reconfigurable-accelerator-for-deep-convolutional-neural-networks">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</h2>

<p>Compared to the <a href="#eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks">Eyeriss v2</a> and <a href="#eyeriss-an-energy-efficient-reconfigurable-accelerator-for-deep-convolutional-neural-networks">Eyeriss energy</a>, this article provides a more detailed explanation on the system architecture of the NoC communication as well as mapping of the PE sets. Also, it introduces an RLC compressed form to reduce the data size as related components in PE architecture.</p>

<p>Plus, this article mentions CNN biases, but there is nothing related to the biases in the architecture it describes.</p>

<hr />

<p>This article introduces the way to reduce the cost of data movement by exploiting data reuse in a multilevel memory hierarchy.  Also, techniques such as compression and data-adaptive processing are introduced to save both memory bandwidth and processing power.</p>

<h3 id="heading-main-features-of-eyeriss">Main Features of Eyeriss</h3>

<ul>
  <li>A four-level memory hierarchy. Data movement can exploit low-cost levels:
    <ul>
      <li>the PE scratch pads
        <ul>
          <li>the inter-PE communication</li>
          <li>the large on-chip global buffer (GLB)</li>
          <li>the off-chip DRAM.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>A CNN dataflow, called Row Stationary (RS), that reconfigures the spatial architecture to map the computation of a given CNN shape and optimize for the best energy efficiency.</li>
  <li>A network-on-chip (NoC) architecture that uses both multicast and point-to-point single-cycle data delivery to support the RS dataflow.</li>
  <li>Run-length compression (RLC) and PE data gating that exploits the statistics of zero data in CNNs to further improve energy efficiency.</li>
</ul>

<p>This is the architecture of Eyeriss system:</p>

<p><img src="https://images-cdn.shimo.im/KUn0Ny1sa6wMuMuy/image.png" alt="Eyeriss system architecture" /></p>

<h3 id="heading-system-control-and-configuration">System Control and Configuration</h3>

<p>The accelerator has two levels of control hierarchy.</p>

<ul>
  <li>The top-level control coordinates:
    <ul>
      <li>traffic between the off-chip DRAM and the GLB through the asynchronous interface;
        <ul>
          <li>traffic between the GLB and the PE array through the NoC;</li>
          <li>operation of the RLC CODEC and ReLU module.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The lower-level control consists of control logic in each PE, which runs independently of each other.</li>
</ul>

<p>The size:</p>

<ul>
  <li>
    <p>spad capacity depends only on the filter row size (S)
but not the ifmap row size (W), and is equal to:
	+ S for a row of filter weights; 
	+ S for a sliding window of ifmap values;
	+ 1 for the psum accumulation.</p>
  </li>
  <li>
    <p>the height and the width of the PE set are equal to the number of filter rows (R) and ofmap rows (E), respectively.</p>
  </li>
</ul>

<p><img src="https://images-cdn.shimo.im/JlEPRdIC1rU50uoT/image.png" alt="Mapping of the PE sets on the spatial array of 168 PEs for the CONV layers in AlexNet" /></p>

<h3 id="heading-exploit-data-statistics">Exploit Data Statistics</h3>

<p>data statistics of CNN is explored to:</p>

<ul>
  <li>reduce DRAM accesses using compression, which is the most energy-consuming data movement per access, on top of the optimized dataflow;</li>
  <li>skip the unnecessary computations to save processing power</li>
</ul>

<h4 id="heading-rlc">RLC</h4>

<p>RLC is used in Eyeriss to exploit the zeros in fmaps and save DRAM bandwidth.</p>

<p><img src="https://uploader.shimo.im/f/Ir9k6AuBWP0NWomF.png!thumbnail" alt="an example of RLC encoding" /></p>

<h4 id="heading-noc">NoC</h4>

<p>Requirements of the NoC architecture:</p>

<ul>
  <li>support the data delivery patterns used in the RS dataflow; should be taken care of:
    <ul>
      <li>different convolution strides (U) result in the ifmap delivery, skipping certain rows in the array;
        <ul>
          <li>a set is divided into segments that are mapped onto different parts of the PE array;</li>
          <li>multiple sets are mapped onto the array simultaneously and different data is required for each set</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>leverage the data reuse achieved by the RS dataflow to further improve energy efficiency;</li>
  <li>provide enough bandwidth for data delivery to support the highly parallel processing in the PE array.</li>
</ul>

<p>Three different types of networks:</p>

<ul>
  <li>Global Input Network: is optimized for a single-cycle multicast from the GLB to a group of PEs that receive the same filter weight, ifmap value, or psum. The challenge is that the group of destination PEs varies across layers due to the differences in data type, convolution stride, and mapping.</li>
  <li>Global Output Network: The GON is used to read the psums generated by a processing pass from the PE array back to the GLB.</li>
  <li>Local Network: pass the psums from two consecutive rows of the same column</li>
</ul>

<p><img src="https://images-cdn.shimo.im/RrYcTIHJCF4DOYXc/image.png" alt="Architecture of the GIN" /></p>

<h4 id="heading-processing-element-and-data-gating">Processing Element and Data Gating</h4>

<p>The datapath is pipelined into three stages: one stage for spad access, and the remaining two for computation.</p>

<p>An extra 12-b Zero Buffer is used to record the position of zeros in the ifmap spad. If a zero ifmap value is detected from the zero buffer, the gating logic will disable the read of the filter spad and prevent the MAC datapath from switching.</p>

<p><img src="https://images-cdn.shimo.im/MPBbaUOFLNEcqibp/image.png" alt="PE architecture" /></p>

<h2 id="heading-some-troubles">Some Troubles</h2>

<h3 id="heading-scalac_2124423">Scalac_2.12.4:4.2.3</h3>

<p>I tried run verilator simulation with the default command</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make <span class="nv">CONFIG</span><span class="o">=</span>SmallBoomAndRocketConfig <span class="nv">BINARY</span><span class="o">=</span>mt-vvadd.riscv run-binary
</code></pre></div></div>

<p>But got the following error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[warn]     Note: Unresolved dependencies path:
[error] sbt.librarymanagement.ResolveException: Error downloading org.scalameta:semanticdb-scalac_2.12.4:4.2.3
[error]   Not found
[error]   Not found
[error]   not found: /home/singularity/.ivy2/local/org.scalameta/semanticdb-scalac_2.12.4/4.2.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/scalameta/semanticdb-scalac_2.12.4/4.2.3/semanticdb-scalac_2.12.4-4.2.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/scalameta/semanticdb-scalac_2.12.4/4.2.3/semanticdb-scalac_2.12.4-4.2.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/scalameta/semanticdb-scalac_2.12.4/4.2.3/semanticdb-scalac_2.12.4-4.2.3.pom
</code></pre></div></div>

<p>When I got into the website it mentioned <code class="language-plaintext highlighter-rouge">https://repo1.maven.org/maven2/org/scalameta/</code> I could only found the version <code class="language-plaintext highlighter-rouge">4.2.3</code> in <code class="language-plaintext highlighter-rouge">https://repo1.maven.org/maven2/org/scalameta/semanticdb-scalac_2.13.1/4.2.3/</code>.</p>

<p>Firstly, I tried to change the Scala version to <code class="language-plaintext highlighter-rouge">2.13.1</code> but occurred some syntax errors when I tried to <code class="language-plaintext highlighter-rouge">sbt publishLocal</code> for <code class="language-plaintext highlighter-rouge">firrtl</code>, so I had to give up that way.</p>

<p>Then, I wanted to find why the default Scala version is <code class="language-plaintext highlighter-rouge">2.12.4</code> though I had set that to <code class="language-plaintext highlighter-rouge">2.12.1</code> in <code class="language-plaintext highlighter-rouge">build.sbt</code>. Then I found the default setting was in <code class="language-plaintext highlighter-rouge">variables.mk:143</code>, so I changed it to <code class="language-plaintext highlighter-rouge">2.12.10</code>, which is the same to that in <code class="language-plaintext highlighter-rouge">firrtl</code>.</p>

<h3 id="heading-no-found-firrlt-object">No found: firrlt object</h3>

<p>After that, I countered with the old trouble: <code class="language-plaintext highlighter-rouge">not found: object firrtl</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[error] /home/singularity/chipyard/tools/chisel3/src/main/scala/chisel3/ChiselExecutionOptions.scala:7:8: not found: object firrtl
[error] import firrtl.{AnnotationSeq, ExecutionOptionsManager, ComposableOptions}
[error]        ^
[error] /home/singularity/chipyard/tools/chisel3/src/main/scala/chisel3/ChiselExecutionOptions.scala:21:44: not found: type ComposableOptions
</code></pre></div></div>

<p>I tried to re-do <code class="language-plaintext highlighter-rouge">publishLocal</code> in almost all components in <code class="language-plaintext highlighter-rouge">chipyard/tools</code> but failed.</p>

<p>Then, Jiuyang told me that I was supposed to <code class="language-plaintext highlighter-rouge">sbt publishLocal</code> then <code class="language-plaintext highlighter-rouge">sbt update</code> in these three directories in order:</p>

<ul>
  <li>firrtl</li>
  <li>chisel3</li>
  <li>rocket-chip</li>
</ul>

<p>But while I tried to <code class="language-plaintext highlighter-rouge">publishLocal</code> under rocket-chip, I found the version of firrtl it need was 1.2 while I only had 1.3 version. So I re-do this flow with the correctness of <code class="language-plaintext highlighter-rouge">version := "1.2-SNAPSHOT"</code>  in <code class="language-plaintext highlighter-rouge">build.sbt</code> under firrtl.</p>

<p>Although I published the rocket-chip, I could not build chipyard successfully.</p>

                    </div>
                </div>
            </div>
            <footer class="unit-foot">
                <div class="unit-inner unit-foot-inner">
                    <div class="post-buttons">
                        <a class="internal gotop" href="#page" title="Back to Top">Back to Top</a>
                        
                    </div>
                    <nav class="pagination">
                        
                            <a class="internal" rel="prev" href="/blog/2019/12/22/2019-12-16-22-weekly-review/" title=" '2019/12/16-22 weekly review'"> ← 2019/12/16-22 weekly review</a>
                        
                        
                            <a class="internal" rel="next" href="/blog/2020/01/05/2019-12-30-2020-01-05-weekly-review/" title="Next Post '2019/12/30-2020/01/05 weekly review'">2019/12/30-2020/01/05 weekly review → </a>
                        
                    </nav>
                </div>
            </footer>
            <div class="misc-content">
                
                
                    <div id="container"></div>
                    <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
                    <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
                    <script>
                    var gitment = new Gitment({
                      owner: "singularitykchen",
                      repo: "https://github.com/SingularityKChen/singularitykchen.github.io",
                      oauth: {
                        client_id: "d22349012c96f951207c",
                        client_secret: "843195c3b1f0b2f258f88684767445c2ceae1996",
                      },
                      labels: "2019/12/23-29 weekly review",
                    })
                    gitment.render('container')
                    </script>
                
            </div>
        </div>
    </div>
</article>

                    </div>
                </div>
            </div>
        </div>
        <footer class="the-footer">
    <div class="unit-foot">
        <div class="unit-inner unit-foot-inner">
            <div class="misc vcard">
                <div class="about">
                    <h4><a href="/">About</a></h4>
                    
                        <p>A gem-based responsive simple texture styled <a href="http://jekyllrb.com/">Jekyll</a> theme.</p>
                    
                    <p><small>Theme <a href="https://github.com/yizeng/jekyll-theme-simple-texture" target="_blank">Simple Texture</a> developed by <a href="https://yizeng.me" target="_blank">Yi Zeng</a>, powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a>.</small></p>
                </div>
                <div class="social-links">
                    
                        <a class="ico-gmail" href="mailto:chency_singularity@163.com" rel="me" target="_blank" title="email"></a>
                    
                    <a class="ico-rss" href="/feed.xml" rel="me" target="_blank" title="feed"></a>
                    
                        
                            <a class="ico-github" href="https://github.com/SingularityKChen" rel="me" target="_blank" title="github"></a>
                        
                    
                        
                            <a class="ico-linkedin" href="https://www.linkedin.com/in/singularitykchen/" rel="me" target="_blank" title="linkedin"></a>
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                </div>
            </div>
        </div>
    </div>
    <a href="#" class="internal back-to-top">Back to Top</a>
</footer>

    </div>

    <script>
$(document).ready(function () {
    var offset = 50,
        duration = 500,
        width = 960;
    $(window).scroll(function () {
        if ($(window).width() > width) {
            if ($(this).scrollTop() > offset) {
                $('footer').css('top', '20px');
                $('footer .back-to-top').fadeIn(duration);
            } else {
                $('footer').css('top', 'auto');
                $('footer .back-to-top').fadeOut(duration);
            }
        }
    });
    $(window).resize(function () {
        if ($(window).width() < width) {
            $('footer').css('top', 'auto');
            $('footer .back-to-top').fadeOut(duration);
        }
        if ($(window).width() >= width && $(this).scrollTop() > offset) {
            $('footer').css('top', '20px');
            $('footer .back-to-top').fadeIn(duration);
        }
    });

    $('footer .back-to-top, .gotop').on('click', function (event) {
        event.preventDefault();
        $('html, body').animate({
            scrollTop: 0
        }, duration);
        return false;
    });

    $('.show-hidden').on('click', function () {
        $(this).parent().next().toggleClass("hidden");
        $(this).toggleClass("hidden");
    });
});
</script>

<!-- Show menu overlay + Jekyll Simple Search option -->
<script src="/assets/javascripts/jekyll-search.jquery.js"></script>
<script>
$(document).ready(function () {
    $('.search-field').simpleJekyllSearch({
      jsonFile: '/search.json',
      template: '<li><a href="{url}">{title} <span class="entry-date"><time datetime="{date}">{shortdate}</time></span></a></li>',
      searchResults: '.search-wrapper .results',
      searchResultsTitle: '<h4>Search results</h4>',
      noResults: '<p>Oh shucks<br/><small>Nothing found :(</small></p>'
    });
});

(function ($, window, undefined) {
    var closeOverlay = function () {
        $('.nav-wrapper, .search-wrapper').removeAttr('style');
        $(".nav-form, .search-form").removeClass('active');
        $("body").removeClass('nav-overlay search-overlay');
    };

    $('.nav-global .btn-search').on('click', function () {
        $('.search-wrapper').css({display: "block"});
        $(".search-form").addClass('active');
        $(".search-form").find('input').focus();
        $("body").addClass('search-overlay');
    });

    $('.nav-global .btn-menu').on('click', function () {
        $('.nav-wrapper').css({display: "block"});
        $(".nav-form").addClass('active');
        $(".nav-form .search-field").prop('disabled', true);
        $("body").addClass('nav-overlay');
    });

    $('.nav-wrapper .btn-close, .search-wrapper .btn-close').on('click', function () {
        closeOverlay();
    });

    $(document).on('keyup', function (e) {
        if (e.keyCode === 27) {
            closeOverlay();
        }
    });
})(jQuery, window);
</script>

<script src='//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-buttons.min.js'></script>
<script src="/assets/javascripts/unveil/jquery.unveil.min.js"></script>

<script>
    window.jQuery.fancybox || document.write('<script src="/assets/javascripts/fancybox/jquery.fancybox.pack.js?v=2.1.4"><\/script>')
    window.jQuery.fancybox.helpers.buttons || document.write('<script src="/assets/javascripts/fancybox/helpers/jquery.fancybox-buttons.js?v=1.0.5"><\/script>')
</script>

<script>
    $("head").append('<link rel="stylesheet" href="/assets/javascripts/fancybox/jquery.fancybox.css?v=2.1.4" type="text/css" />');
    $("head").append('<link rel="stylesheet" href="/assets/javascripts/fancybox/helpers/jquery.fancybox-buttons.css?v=1.0.5" type="text/css" />');
    $(".post-image").fancybox({
        prevEffect: 'none',
        nextEffect: 'none',
        closeBtn: true,
        helpers: {
            title: {
                type: 'float'
            }
        }
    });
    $(document).ready(function () {
        $(".post-image > img").unveil(450);
    });
</script>

</body>

</html>